{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9528724-84a3-46d5-ba5e-01bba6797d01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T07:15:28.779321Z",
     "iopub.status.busy": "2023-03-30T07:15:28.778976Z",
     "iopub.status.idle": "2023-03-30T07:15:58.710080Z",
     "shell.execute_reply": "2023-03-30T07:15:58.709164Z",
     "shell.execute_reply.started": "2023-03-30T07:15:28.779293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.23.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.21.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.12.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.13.1+cu116)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.1.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.9.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.12.1+cu116)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.9/dist-packages (3.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.23.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy) (66.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from pathy>=0.3.5->spacy) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting temporal_taggers\n",
      "  Downloading temporal_taggers-0.0.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from temporal_taggers) (3.9.0)\n",
      "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.9/dist-packages (from temporal_taggers) (1.12.1+cu116)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.9/dist-packages (from temporal_taggers) (4.11.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from temporal_taggers) (2.4.0)\n",
      "Requirement already satisfied: transformers>=4.3.3 in /usr/local/lib/python3.9/dist-packages (from temporal_taggers) (4.21.3)\n",
      "Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.9/dist-packages (from temporal_taggers) (3.4.1)\n",
      "Collecting conllu\n",
      "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.9->temporal_taggers) (2.3.2.post1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (1.23.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (1.0.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (0.4.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (66.1.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (2.28.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (1.9.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (8.1.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (4.64.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (23.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0->temporal_taggers) (2.4.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->temporal_taggers) (4.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.3.3->temporal_taggers) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.3.3->temporal_taggers) (0.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.3.3->temporal_taggers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.3.3->temporal_taggers) (0.12.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->temporal_taggers) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->temporal_taggers) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.9/dist-packages (from datasets->temporal_taggers) (0.3.5.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->temporal_taggers) (2023.1.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->temporal_taggers) (1.5.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->temporal_taggers) (10.0.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->temporal_taggers) (0.70.13)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->temporal_taggers) (3.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.9/dist-packages (from seqeval->temporal_taggers) (1.1.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->temporal_taggers) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->temporal_taggers) (18.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->temporal_taggers) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->temporal_taggers) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->temporal_taggers) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->temporal_taggers) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->temporal_taggers) (4.0.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from pathy>=0.3.5->spacy>=3.0->temporal_taggers) (6.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->temporal_taggers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->temporal_taggers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->temporal_taggers) (1.26.14)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval->temporal_taggers) (1.9.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval->temporal_taggers) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval->temporal_taggers) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0->temporal_taggers) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0->temporal_taggers) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0->temporal_taggers) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy>=3.0->temporal_taggers) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->temporal_taggers) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->temporal_taggers) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->temporal_taggers) (1.14.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16164 sha256=848e6a4ea14c1370914b903ca2b873ad8429f0810af2329c2de4d7eb2e57a7fb\n",
      "  Stored in directory: /root/.cache/pip/wheels/9c/d6/00/1ccfd5a7466a94774e00022683d4b028836032dfb85007822b\n",
      "Successfully built seqeval\n",
      "Installing collected packages: conllu, seqeval, temporal_taggers\n",
      "Successfully installed conllu-4.5.2 seqeval-1.2.2 temporal_taggers-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gensim\n",
      "  Downloading gensim-4.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.9.2)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.20.9\n",
      "  Downloading Levenshtein-0.20.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.20.9 python-Levenshtein-0.20.9 rapidfuzz-2.13.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install transformers\n",
    "!pip install spacy\n",
    "!pip install temporal_taggers\n",
    "!pip install gensim\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158d19ef-6a02-4cea-86e5-782bea03c752",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T07:15:58.712006Z",
     "iopub.status.busy": "2023-03-30T07:15:58.711738Z",
     "iopub.status.idle": "2023-03-30T07:16:08.809030Z",
     "shell.execute_reply": "2023-03-30T07:16:08.808298Z",
     "shell.execute_reply.started": "2023-03-30T07:15:58.711980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-sm==3.4.1) (3.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.9.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (66.1.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c76c82d-ea02-4a7f-9ea5-85f7b327030c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T18:28:25.952428Z",
     "iopub.status.busy": "2023-03-29T18:28:25.951984Z",
     "iopub.status.idle": "2023-03-29T18:28:50.044948Z",
     "shell.execute_reply": "2023-03-29T18:28:50.043970Z",
     "shell.execute_reply.started": "2023-03-29T18:28:25.952399Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_embeddings_to_file(embedding_index, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(embedding_index, f)\n",
    "\n",
    "def load_embeddings_from_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        embedding_index = pickle.load(f)\n",
    "    return embedding_index\n",
    "\n",
    "embedding_file = '/notebooks/glove.6B.300d.txt'\n",
    "pickle_file_path = '/notebooks/embedding_index.pkl'\n",
    "\n",
    "# Load the embeddings from the text file and create the embedding_index\n",
    "embedding_index = {}\n",
    "with open(embedding_file, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vector\n",
    "\n",
    "# Save the embedding_index to a pickle file\n",
    "save_embeddings_to_file(embedding_index, pickle_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640d87e9-d38c-4937-be3d-1056e9ac8caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T07:16:35.078926Z",
     "iopub.status.busy": "2023-03-30T07:16:35.078561Z",
     "iopub.status.idle": "2023-03-30T07:17:38.373860Z",
     "shell.execute_reply": "2023-03-30T07:17:38.373121Z",
     "shell.execute_reply.started": "2023-03-30T07:16:35.078894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e4bb9616294c1ca132f68be1191fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1d8e9831064c9a98c08ee2b140cb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/415M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2f6d686bed464d92fa143722cfcf86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410d82978b0746ada01c74a9ec63e015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7cd648e48a40989498f6f092c27e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc14fcf080e498ea0ee1a79b321fdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97e9e11acde4f1980a1c2b0b3033f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e44492c426e4abe921507cd23a1c56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c81e4733bd4eba8a1cc661c4aa96c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b868d40a0bc04b448763d63edb5ec636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c47d4a5bb5e4148a28d044f1f627d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3a33e16f56431fb929ea152c886306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5965a121d1c04f4db69c92139d157016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e53772218c4496ba31c57067e83435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043f088b2e034217b3e863d5949c155b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654379ba0bb5480d9f82e4e2f0191cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6040545184ab4950b6b7cae6d8429517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232a30cbee004261b2c606d2cef4b536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80329a5dd7f14a3cb1ba7fa8f970a716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43baff1ca0d41c091c09e31dcc56cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/952 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bbe9517b774de196960d9331ca2c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2dc9ec3df64e4d914d16939160e04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb23c3fd0aa414a996a66f6be66ba27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spm.model:   0%|          | 0.00/2.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "from temporal_taggers.evaluation import merge_tokens\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from itertools import chain\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"satyaalmasian/temporal_tagger_BERT_tokenclassifier\")\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"satyaalmasian/temporal_tagger_BERT_tokenclassifier\", use_fast=False)\n",
    "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# Load the classifier pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"microsoft/deberta-v2-xlarge-mnli\", top_k=None, framework=\"pt\", device=0, batch_size=100)\n",
    "glove_model = KeyedVectors.load_word2vec_format('glove.6B.300d.word2vec.bin', binary=True)\n",
    "\n",
    "# Load embeddings\n",
    "def load_embeddings_from_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        embedding_index = pickle.load(f)\n",
    "    return embedding_index\n",
    "\n",
    "pickle_file_path = '/notebooks/embedding_index.pkl'\n",
    "\n",
    "# Load the embeddings from the text file and create the embedding_index\n",
    "embedding_index = load_embeddings_from_file(pickle_file_path)\n",
    "\n",
    "            \n",
    "def predict(df):\n",
    "    # Load json and convert to dataframe foramt\n",
    "    # Do Basic Cleaning, also drop any empty or NA rows\n",
    "    # We do not remove duplicates, since there might be use cases that require duplicate predictions (but the label will be the same..)\n",
    "    start_time = time.time()\n",
    "    df = drop_empty_rows(df, \"claim\", \"evidence\")\n",
    "\n",
    "    ## Apply preprocessing function to claim and evidence_content columns of dataframe\n",
    "    df['claim'] = df['claim'].apply(preprocess_string)\n",
    "    df['evidence'] = df['evidence'].apply(preprocess_string)\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Use nlp.pipe for efficient processing of multiple texts\n",
    "    claims = list(df['claim'])\n",
    "    evidences = list(df['evidence'])\n",
    "\n",
    "    # Set batch_size and n_process for faster processing\n",
    "    batch_size = 500\n",
    "    n_process = -1  # Use all available cores\n",
    "\n",
    "    # Process claims and evidences using nlp.pipe with batch_size and n_process\n",
    "    claims_docs = list(nlp.pipe(claims, batch_size=batch_size, n_process=n_process))\n",
    "    evidences_docs = list(nlp.pipe(evidences, batch_size=batch_size, n_process=n_process))\n",
    "\n",
    "    # Extract entities for all docs\n",
    "    claim_named_entities_list, claim_entities, claim_chunks, claim_temporal = extract_entities_and_chunks(claims_docs)\n",
    "    evidence_named_entities_list, evidence_entities, evidence_chunks, evidence_temporal = extract_entities_and_chunks(evidences_docs)\n",
    "\n",
    "    df['claim_named_entities'] = claim_named_entities_list\n",
    "    df['claim_entities'] = claim_entities\n",
    "    df['claim_chunks'] = claim_chunks\n",
    "    df['evidences_named_entities'] = evidence_named_entities_list\n",
    "    df['evidence_entities'] = evidence_entities\n",
    "    df['evidence_chunks'] = evidence_chunks\n",
    "\n",
    "    df['claim_temporal'] = claim_temporal\n",
    "    df['evidence_temporal'] = evidence_temporal\n",
    "\n",
    "    # Set batch size and number of batches\n",
    "    batch_size = 500\n",
    "    num_batches = len(df) // batch_size + 1\n",
    "\n",
    "    # Initialize an empty list to store the similarity scores\n",
    "    similarity_scores = []\n",
    "\n",
    "    # Compute the similarity score for each row in the dataframe using batching\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "\n",
    "        claims = list(batch_df['claim'])\n",
    "        evidences = list(batch_df['evidence'])\n",
    "\n",
    "        # Compute the similarity scores for the batch using the `compute_similarity()` function\n",
    "        batch_scores = []\n",
    "        for j, claim in enumerate(claims):\n",
    "            doc_scores = compute_similarity(claim, evidences[j])\n",
    "            batch_scores.append(doc_scores)\n",
    "\n",
    "        similarity_scores += batch_scores\n",
    "\n",
    "    # Flatten the similarity scores list\n",
    "    similarity_scores = [score for batch_scores in similarity_scores for score in batch_scores]\n",
    "\n",
    "    # Add the similarity score as a new column to the dataframe\n",
    "    df['similarity_score'] = similarity_scores\n",
    "    \n",
    "    ## Add Deberta V2\n",
    "    # Split the data into batches and process each batch\n",
    "    batch_size = 100\n",
    "    dfs = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        result = add_probability_columns(batch)\n",
    "        dfs.append(result)\n",
    "\n",
    "    # Concatenate the results\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    ## Add counts of named entities only\n",
    "    df['claim_named_entities'] = df['claim_named_entities'].astype(str).apply(eval)\n",
    "    df['evidences_named_entities'] = df['evidences_named_entities'].astype(str).apply(eval)\n",
    "\n",
    "    df['count_claim_named_entities'] = df['claim_named_entities'].apply(count_list_items)\n",
    "    df['count_evidence_named_entities'] = df['evidences_named_entities'].apply(count_list_items)\n",
    "    \n",
    "    # Take the probabilities out of the brackets\n",
    "    df['contradiction_prob'] = df['contradiction_prob'].astype(str).apply(eval)\n",
    "    df['neutral_prob'] = df['neutral_prob'].astype(str).apply(eval)\n",
    "    df['entailment_prob'] = df['entailment_prob'].astype(str).apply(eval)\n",
    "\n",
    "    # apply function to DataFrame column\n",
    "    df['contradiction_prob'] = df['contradiction_prob'].apply(extract_float)\n",
    "    df['neutral_prob'] = df['neutral_prob'].apply(extract_float)\n",
    "    df['entailment_prob'] = df['entailment_prob'].apply(extract_float)\n",
    "    \n",
    "    # Add similarity metrics between entities, chunks, and temporal phrases, load glove from binary file\n",
    "\n",
    "    # Prepare data for parallel processing\n",
    "    data = range(len(df))\n",
    "\n",
    "    # Run the function in parallel using a ThreadPoolExecutor\n",
    "    partial_row_similarity = partial(row_similarity, df)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        similarity_scores = list(executor.map(partial_row_similarity, data))\n",
    "\n",
    "    # Unpack the chunk_similarity and entity_similarity scores\n",
    "    chunk_similarity, entity_similarity, named_entity_similarity, temporal_similarity = zip(*similarity_scores)\n",
    "\n",
    "    # Add similarity scores to the DataFrame\n",
    "    df['chunks_similarity'] = chunk_similarity\n",
    "    df['entities_similarity'] = entity_similarity\n",
    "    df['named_entity_similarity'] = named_entity_similarity\n",
    "    df['temporal_similarity'] = temporal_similarity\n",
    "    \n",
    "    # Add embedding\n",
    "    # Apply the compute_average_embedding function to the claim and evidence columns to obtain the average word embeddings\n",
    "    df['claim_embedding'] = df['claim'].apply(compute_average_embedding)\n",
    "    df['evidence_embedding'] = df['evidence'].apply(compute_average_embedding)\n",
    "\n",
    "    processed_df = df.iloc[:, 10:]\n",
    "    processed_df = processed_df.rename(columns={'chunks_similarity': 'all_chunks_similarity', 'named_entity_similarity': 'named_entities_similarity', 'entities_similarity': 'all_entities_similarity'})\n",
    "    processed_df# reorder columns\n",
    "    new_order = ['contradiction_prob', 'neutral_prob', 'entailment_prob','count_claim_named_entities', 'count_evidence_named_entities',\n",
    "           'similarity_score', 'all_entities_similarity',\n",
    "           'all_chunks_similarity', 'named_entities_similarity',\n",
    "           'temporal_similarity', 'claim_embedding', \"evidence_embedding\"] \n",
    "    processed_df = processed_df.reindex(columns=new_order)\n",
    "\n",
    "    # Standardize the numerical features\n",
    "    scaler = StandardScaler()\n",
    "    processed_df_scaled = scaler.fit_transform(processed_df.iloc[:, :10])\n",
    "    # Combine the standardized numerical features with the claim and evidence embeddings\n",
    "    claim_embeddings = np.vstack(processed_df['claim_embedding'].values)\n",
    "    evidence_embeddings = np.vstack(processed_df['evidence_embedding'].values)\n",
    "    processed_df_scaled_emb = np.hstack((processed_df_scaled, claim_embeddings, evidence_embeddings))\n",
    "\n",
    "    # Load the model from file\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.load_model(\"xgboost_final_reduced.model\")\n",
    "    # Make predictions on new data\n",
    "    predictions = model.predict(processed_df_scaled_emb)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n",
    "    return predictions\n",
    "\n",
    "def drop_empty_rows(df, col1, col2):\n",
    "    # Drop rows with NaN or null values in col1 and col2\n",
    "    df = df.dropna(subset=[col1, col2], how='any')\n",
    "\n",
    "    # Drop rows with empty strings in col1 and col2\n",
    "    df = df[~df[col1].str.strip().eq('')]\n",
    "    df = df[~df[col2].str.strip().eq('')]\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def preprocess_string(text):\n",
    "    # Remove unnecessary whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Remove non-alphanumeric characters except for whitespace and normal punctuation\n",
    "    text = re.sub(r'[^\\w\\s\\'\",.?!:;\\-]|[\\x80-\\xff]', '', text)\n",
    "\n",
    "    # Remove consecutive punctuation\n",
    "    text = re.sub(r'([,.?!:;\\-])\\1+', r'\\1', text)\n",
    "\n",
    "    # Remove leading and trailing punctuation except for normal end-of-sentence punctuation\n",
    "    text = re.sub(r'^(\\'|\"|\\(|\\[|\\{)*\\s*(.*?)[\\s\\'\",:;!?]*(\\.|\\?|\\!)(\\'|\"|\\)|\\]|\\})*\\s*$', r'\\2\\3', text)\n",
    "\n",
    "    # Remove spaces before commas and full stops\n",
    "    text = re.sub(r'\\s+([,.])', r'\\1', text)\n",
    "\n",
    "    # Fix possessive apostrophe\n",
    "    text = re.sub(r'\\s+\\'s\\b', '\\'s', text)\n",
    "\n",
    "    # Remove unnecessary quotes and spaces\n",
    "    text = re.sub(r'\\'\\'|\\s*\"\\s*|\\s*\\'', ' ', text).strip()\n",
    "\n",
    "    # Remove all remaining extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Define a function to extract all entities, and noun chunks\n",
    "def extract_entities_and_chunks(docs):\n",
    "    named_entities_list = []\n",
    "    entities_list = []\n",
    "    chunks_list = []\n",
    "    temporal_list = []\n",
    "    for doc in docs:\n",
    "        # Extract entities and noun chunks\n",
    "        entities = [ent.text for ent in doc.ents if ent.label_ not in [\"DATE\", \"TIME\"]]\n",
    "        entities_list.append(entities)\n",
    "        noun_chunks = [chunk.text for chunk in doc.noun_chunks if not any(ent.label_ in [\"DATE\", \"TIME\"] for ent in chunk.ents)]\n",
    "        chunks_list.append(noun_chunks)\n",
    "\n",
    "        # Extract named entities \n",
    "        named_entities = [ent.text for ent in doc.ents]\n",
    "        named_entities_list.append(named_entities)\n",
    "\n",
    "        # Extract temporal phrases\n",
    "        text = doc.text\n",
    "        temporal_phrases = extract_temporal_phrases(text)\n",
    "        temporal_list.append(temporal_phrases)\n",
    "\n",
    "\n",
    "    # Return the named entities, entities and noun chunks and temporal phrases \n",
    "    return named_entities_list, entities_list, chunks_list, temporal_list\n",
    "\n",
    "# Define a function to extract temporal elements\n",
    "def extract_temporal_phrases(text):\n",
    "    id2label = {v: k for k, v in model.config.label2id.items()}\n",
    "    text = text.lower()\n",
    "    processed_text = text_tokenizer(text, return_tensors=\"pt\")\n",
    "    result = model(**processed_text)\n",
    "    classification = torch.argmax(result[0], dim=2)\n",
    "    merged_tokens = merge_tokens(processed_text[\"input_ids\"][0], classification[0], id2label, text_tokenizer)\n",
    "    temporal_phrases = []\n",
    "    temporal = False\n",
    "    for token in merged_tokens:\n",
    "      if token[1] == \"DATE\" or token[1] == \"TIME\" or token[1] == \"DURATION\":\n",
    "          if temporal:\n",
    "              temporal_phrases[-1] += \" \" + token[0]\n",
    "          else:\n",
    "              temporal_phrases.append(token[0])\n",
    "              temporal = True\n",
    "      else:\n",
    "          temporal = False\n",
    "\n",
    "    temporal_phrases = [str(re.sub(r'[^\\w\\s]', '', phrase.strip())) for phrase in temporal_phrases]\n",
    "    temporal_phrases = list(set([re.sub(r'\\s+', ' ', phrase.strip()) for phrase in temporal_phrases]))\n",
    "\n",
    "    return temporal_phrases\n",
    "\n",
    "# Define a function to compute the similarity score between a query and a document\n",
    "def compute_similarity(query, doc):\n",
    "    # Encode the query and document\n",
    "    query_emb = sentence_model.encode(query)\n",
    "    doc_emb = sentence_model.encode(doc)\n",
    "\n",
    "    #Compute dot score between query and all document embeddings\n",
    "    scores = util.dot_score(query_emb, doc_emb)[0].tolist()\n",
    "\n",
    "    return scores\n",
    "\n",
    "def add_probability_columns(df):\n",
    "    # Define input pairs from DataFrame\n",
    "    input_pairs = list(zip(df['claim'].tolist(), df['evidence'].tolist()))\n",
    "\n",
    "    # Combine input pairs into single strings with [CLS] and [SEP] tokens\n",
    "    inputs = [f\"[CLS]{pair[0]} [SEP] {pair[1]} [SEP]\" for pair in input_pairs]\n",
    "\n",
    "    # Run classification on batch\n",
    "    results = classifier(inputs, max_length=128)\n",
    "\n",
    "    # Initialize lists for each label's probability\n",
    "    contradiction_prob = [[] for _ in range(len(df))]\n",
    "    neutral_prob = [[] for _ in range(len(df))]\n",
    "    entailment_prob = [[] for _ in range(len(df))]\n",
    "\n",
    "    # Extract probability scores for each input pair\n",
    "    for i, result in enumerate(results):\n",
    "        for score in result:\n",
    "            if score['label'] == 'CONTRADICTION':\n",
    "                contradiction_prob[i].append(score['score'])\n",
    "            elif score['label'] == 'NEUTRAL':\n",
    "                neutral_prob[i].append(score['score'])\n",
    "            elif score['label'] == 'ENTAILMENT':\n",
    "                entailment_prob[i].append(score['score'])\n",
    "\n",
    "    # Add probability columns to original DataFrame\n",
    "    df['contradiction_prob'] = contradiction_prob\n",
    "    df['neutral_prob'] = neutral_prob\n",
    "    df['entailment_prob'] = entailment_prob\n",
    "\n",
    "    return df\n",
    "\n",
    "def count_list_items(s):\n",
    "    return (len(s))\n",
    "\n",
    "def extract_float(lst):\n",
    "    return lst[0]\n",
    "\n",
    "def normalized_levenshtein_distance(a, b):\n",
    "    max_length = max(len(a), len(b))\n",
    "    if max_length == 0:\n",
    "        return 0\n",
    "    return levenshtein_distance(a, b) / max_length\n",
    "\n",
    "def preprocess_keyword(keyword):\n",
    "    keyword = re.sub(r'[^A-Za-z0-9 ]+', '', keyword)  # Remove non-alphanumeric characters except space\n",
    "    keyword = re.sub(' +', ' ', keyword)\n",
    "    keyword = keyword.strip()  # Remove leading and trailing spaces\n",
    "    return keyword.split(\" \")  # Return the preprocessed keyword\n",
    "\n",
    "def avg_similarity(claim_keywords, evidence_keywords, model, alpha=0.5):\n",
    "    total_similarity = 0\n",
    "    count = 0\n",
    "\n",
    "   # Preprocess claim keywords and evidence keywords\n",
    "    claim_keywords = [word for keyword in claim_keywords for word in preprocess_keyword(keyword)]\n",
    "    evidence_keywords = [word for keyword in evidence_keywords for word in preprocess_keyword(keyword)]\n",
    "\n",
    "    for claim_keyword in claim_keywords:\n",
    "        max_similarity = -1\n",
    "\n",
    "        for evidence_keyword in evidence_keywords:\n",
    "            # Check if words are in the model vocabulary\n",
    "            claim_in_vocab = claim_keyword in model\n",
    "            evidence_in_vocab = evidence_keyword in model\n",
    "\n",
    "            embedding_similarity = 0\n",
    "\n",
    "            if claim_in_vocab and evidence_in_vocab:\n",
    "              # Calculate the cosine similarity using word embeddings\n",
    "              embedding_similarity = cosine_similarity([model[claim_keyword]], [model[evidence_keyword]])\n",
    "\n",
    "            # Calculate the normalized Levenshtein distance\n",
    "            levenshtein_sim = 1 - normalized_levenshtein_distance(claim_keyword, evidence_keyword)\n",
    "\n",
    "            # Combine the embedding similarity and Levenshtein similarity with a weighted average\n",
    "            combined_similarity = alpha * embedding_similarity + (1 - alpha) * levenshtein_sim\n",
    "\n",
    "            max_similarity = max(max_similarity, combined_similarity)\n",
    "\n",
    "        total_similarity += max_similarity\n",
    "        count += 1\n",
    "\n",
    "    return total_similarity / count if count > 0 else 0\n",
    "\n",
    "# Function to apply the avg_similarity on DataFrame rows\n",
    "def row_similarity(df, index):\n",
    "    row = df.loc[index]\n",
    "    claim_chunks = row['claim_chunks']\n",
    "    evidence_chunks = row['evidence_chunks']\n",
    "    chunk_similarity = avg_similarity(claim_chunks, evidence_chunks, glove_model, alpha=0.5)\n",
    "\n",
    "    claim_entities = row['claim_entities']\n",
    "    evidence_entities = row['evidence_entities']\n",
    "    entity_similarity = avg_similarity(claim_entities, evidence_entities, glove_model, alpha=0.5)\n",
    "\n",
    "    claim_named_entities = row['claim_named_entities']\n",
    "    evidence_named_entities = row['evidences_named_entities']\n",
    "    named_entity_similarity = avg_similarity(claim_named_entities, evidence_named_entities, glove_model, alpha=0.5)\n",
    "\n",
    "    claim_temporal = row['claim_temporal']\n",
    "    evidence_temporal = row['evidence_temporal']\n",
    "    temporal_similarity = avg_similarity(claim_temporal, evidence_temporal, glove_model, alpha=0.5)\n",
    "\n",
    "    return chunk_similarity, entity_similarity, named_entity_similarity, temporal_similarity\n",
    "\n",
    "# Define a function to compute the average word embedding for a sentence\n",
    "def compute_average_embedding(sentence):\n",
    "    words = sentence.split()\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in embedding_index:\n",
    "            embeddings.append(embedding_index[word])\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677b302-ff3b-4530-867e-bd643125a007",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T07:18:26.571988Z",
     "iopub.status.busy": "2023-03-30T07:18:26.571066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# We assume here that data is passed in the correct format\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/notebooks/trial_preprocessing.csv\", encoding='utf-8')\n",
    "predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ea643-f2ce-4e1a-bdc0-70b1103f423b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
